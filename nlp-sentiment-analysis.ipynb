{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# IMDB Sentiment Analysis — Dual Pipeline (TF‑IDF + BiLSTM)\n\nThis notebook builds an end‑to‑end **IMDB review classifier** with two complementary approaches:\n\n1) **Classical ML**: TF‑IDF (word + char) → Logistic Regression (5‑fold CV)\n2) **Deep Learning**: Tokenizer + **BiLSTM** (5‑fold CV with EarlyStopping)\n\n**Preprocessing uses NLTK + spaCy** (tokenization, cleaning, lemmatization) with graceful fallbacks.\nFinally, we do a brief **error analysis** (negation/sarcasm) and a tiny **inference** function.\n\n> Target metrics (indicative, dataset & runtime dependent): Accuracy ≈ 0.90, F1 ≈ 0.88\n","metadata":{}},{"cell_type":"markdown","source":"\n## How to Run (Kaggle or Local)\n\n**Dataset options** (pick one):\n- Kaggle: Add **“IMDB Dataset of 50K Movie Reviews”** to the notebook (usually at\n  `/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv`).\n- Local: Put `IMDB Dataset.csv` next to this notebook.\n\n> If the CSV is missing, the notebook stops with a clear message.\n\n**Compute**:\n- For the BiLSTM, enable **GPU** (e.g., Kaggle → *Settings* → *Accelerator* → GPU). CPU still works, just slower.\n\n**Libraries**:\n- We rely on common Kaggle defaults: `numpy`, `pandas`, `scikit-learn`, `tensorflow/keras`, `nltk`, `spacy`.\n- If `en_core_web_sm` isn’t available, we fall back to `spacy.blank(\"en\")` and use **NLTK WordNetLemmatizer**.\n","metadata":{}},{"cell_type":"code","source":"\n# === Imports & Setup ===\nimport os, re, gc, sys, math, json, random, string, html\nfrom pathlib import Path\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.utils import shuffle\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Try spaCy gracefully\ntry:\n    import spacy\n    try:\n        nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"textcat\"])\n        SPACY_MODE = \"en_core_web_sm\"\n    except Exception:\n        nlp = spacy.blank(\"en\")\n        SPACY_MODE = \"blank_en\"\nexcept Exception as e:\n    spacy = None\n    nlp = None\n    SPACY_MODE = \"not_available\"\n\n# TensorFlow / Keras for the LSTM model\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPool1D\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n\nprint(\"Python:\", sys.version)\nprint(\"TF:\", tf.__version__)\nprint(\"spaCy:\", spacy.__version__ if spacy else \"N/A\", \"| mode:\", SPACY_MODE)\n\n# Ensure NLTK data\ntry:\n    nltk.data.find(\"corpora/stopwords\")\nexcept LookupError:\n    nltk.download(\"stopwords\")\ntry:\n    nltk.data.find(\"corpora/wordnet\")\nexcept LookupError:\n    nltk.download(\"wordnet\")\ntry:\n    nltk.data.find(\"tokenizers/punkt\")\nexcept LookupError:\n    nltk.download(\"punkt\")\n\nEN_STOP = set(stopwords.words(\"english\"))\nWN_LEMM = WordNetLemmatizer()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:00:39.349307Z","iopub.execute_input":"2025-10-17T16:00:39.349549Z","iopub.status.idle":"2025-10-17T16:01:02.299323Z","shell.execute_reply.started":"2025-10-17T16:00:39.349525Z","shell.execute_reply":"2025-10-17T16:01:02.298432Z"}},"outputs":[{"name":"stderr","text":"2025-10-17 16:00:51.574307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760716851.733620      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760716851.786378      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nTF: 2.18.0\nspaCy: 3.8.7 | mode: en_core_web_sm\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# === Load IMDB Dataset (CSV) ===\n# Expected columns: 'review', 'sentiment' where sentiment ∈ {'positive', 'negative'}\n\nPOSSIBLE_PATHS = [\n    \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\",\n    \"./IMDB Dataset.csv\",\n    \"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\",\n]\n\ndata_path = None\nfor p in POSSIBLE_PATHS:\n    if os.path.exists(p):\n        data_path = p\n        break\n\nif not data_path:\n    raise FileNotFoundError(\n        \"IMDB Dataset CSV not found. Please add the Kaggle dataset \"\n        \"('IMDB Dataset of 50K Movie Reviews') or place 'IMDB Dataset.csv' beside the notebook.\"\n    )\n\ndf = pd.read_csv(data_path)\ndf = df.rename(columns={c: c.strip().lower() for c in df.columns})\nassert \"review\" in df.columns and \"sentiment\" in df.columns, \"CSV must have 'review' and 'sentiment' columns.\"\n\n# Map labels to {0,1}\nlabel_map = {\"negative\": 0, \"positive\": 1}\ndf[\"label\"] = df[\"sentiment\"].map(label_map).astype(int)\n\nprint(df.shape, df[\"label\"].value_counts())\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:55:03.515697Z","iopub.execute_input":"2025-10-17T16:55:03.515978Z","iopub.status.idle":"2025-10-17T16:55:04.935463Z","shell.execute_reply.started":"2025-10-17T16:55:03.515955Z","shell.execute_reply":"2025-10-17T16:55:04.934849Z"}},"outputs":[{"name":"stdout","text":"(50000, 3) label\n1    25000\n0    25000\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  label\n0  One of the other reviewers has mentioned that ...  positive      1\n1  A wonderful little production. <br /><br />The...  positive      1\n2  I thought this was a wonderful way to spend ti...  positive      1\n3  Basically there's a family where a little boy ...  negative      0\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"\n## Preprocessing (NLTK + spaCy)\n\nSteps:\n- Lowercasing, HTML tag & URL removal, punctuation normalization\n- Tokenization with **spaCy** if available; otherwise NLTK/regex\n- Lemmatization: spaCy if model present, else **NLTK WordNet** lemmatizer\n- Stopword removal (keeps negations like *not, n't, never*)\n","metadata":{}},{"cell_type":"code","source":"\n# === Text cleaning utilities ===\n\nCONTRACTIONS = {\n    \"can't\": \"can not\", \"won't\": \"will not\", \"n't\": \" not\",\n    \"i'm\": \"i am\", \"it's\": \"it is\", \"that's\": \"that is\",\n    \"there's\": \"there is\", \"what's\": \"what is\", \"you're\": \"you are\",\n    \"they're\": \"they are\", \"we're\": \"we are\", \"i've\": \"i have\",\n    \"don't\": \"do not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n    \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n    \"shouldn't\": \"should not\", \"wouldn't\": \"would not\", \"couldn't\": \"could not\",\n    \"mustn't\": \"must not\", \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\"\n}\n\nNEGATION_TOKENS = {\"not\", \"no\", \"never\", \"n't\"}\n\nPUNCT_TABLE = str.maketrans({c: f\" {c} \" for c in string.punctuation})\n\ndef expand_contractions(text: str) -> str:\n    text_low = text.lower()\n    for k, v in CONTRACTIONS.items():\n        text_low = text_low.replace(k, v)\n    return text_low\n\ndef basic_clean(text: str) -> str:\n    text = html.unescape(str(text))\n    text = re.sub(r\"<br\\s*/?>\", \" \", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<[^>]+>\", \" \", text)  # strip HTML tags\n    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)  # URLs\n    text = expand_contractions(text)\n    text = text.translate(PUNCT_TABLE)  # space-pad punctuation\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef spacy_tokenize_lemma(text: str):\n    doc = nlp(text) if nlp else None\n    if doc is None:\n        return None\n    toks = []\n    for t in doc:\n        tok = (t.lemma_ if t.lemma_ else t.text).lower().strip()\n        if tok and tok not in EN_STOP and not tok.isdigit():\n            # keep negations\n            if tok in NEGATION_TOKENS or tok.isalpha():\n                toks.append(tok)\n    return toks\n\ndef nltk_tokenize_lemma(text: str):\n    words = re.findall(r\"[a-zA-Z']+\", text.lower())\n    out = []\n    for w in words:\n        if w in EN_STOP and w not in NEGATION_TOKENS:\n            continue\n        lemma = WN_LEMM.lemmatize(w)\n        if lemma:\n            out.append(lemma)\n    return out\n\ndef preprocess_text(text: str):\n    text = basic_clean(text)\n    # Try spaCy pipeline first (will use blank(\"en\") if small model missing)\n    toks = spacy_tokenize_lemma(text) if nlp is not None else None\n    if not toks:\n        toks = nltk_tokenize_lemma(text)\n    return \" \".join(toks)\n\n# Quick smoke test\nprint(preprocess_text(\"I didn't like this movie at all — it's not good! But performances weren't bad.\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:55:09.699450Z","iopub.execute_input":"2025-10-17T16:55:09.699759Z","iopub.status.idle":"2025-10-17T16:55:09.727802Z","shell.execute_reply.started":"2025-10-17T16:55:09.699739Z","shell.execute_reply":"2025-10-17T16:55:09.727169Z"}},"outputs":[{"name":"stdout","text":"like movie good performance bad\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n# This can take a couple of minutes; feel free to subsample while experimenting.\ndf[\"text\"] = df[\"review\"].astype(str).apply(preprocess_text)\ndf = df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\nprint(df.shape)\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T16:55:12.609871Z","iopub.execute_input":"2025-10-17T16:55:12.610150Z","iopub.status.idle":"2025-10-17T17:08:18.960610Z","shell.execute_reply.started":"2025-10-17T16:55:12.610128Z","shell.execute_reply":"2025-10-17T17:08:18.959951Z"}},"outputs":[{"name":"stdout","text":"(50000, 2)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  one reviewer mention watch oz episode hook rig...      1\n1  wonderful little production filming technique ...      1\n2  think wonderful way spend time hot summer week...      1\n3  basically family little boy jake think zombie ...      0\n4  petter mattei love time money visually stunnin...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>one reviewer mention watch oz episode hook rig...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>wonderful little production filming technique ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>think wonderful way spend time hot summer week...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>basically family little boy jake think zombie ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>petter mattei love time money visually stunnin...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"\n## Model A — TF‑IDF + Logistic Regression (5‑fold CV)\n\nWe use both **word n‑grams (1–2)** and **character n‑grams (3–5)** for robustness to misspellings.\n","metadata":{}},{"cell_type":"code","source":"\ntfidf_word = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=200_000, sublinear_tf=True)\ntfidf_char = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2, max_features=100_000, sublinear_tf=True)\n\ndef build_features(texts):\n    Xw = tfidf_word.fit_transform(texts)\n    Xc = tfidf_char.fit_transform(texts)\n    from scipy.sparse import hstack\n    X = hstack([Xw, Xc]).tocsr()\n    return X\n\nX = build_features(df[\"text\"])\ny = df[\"label\"].values\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\noof_pred = np.zeros(len(df))\nfold_metrics = []\n\nfor fold, (tr, va) in enumerate(skf.split(X, y), 1):\n    clf = LogisticRegression(\n        max_iter=2000,\n        n_jobs=-1,\n        solver=\"saga\",\n        class_weight=\"balanced\",\n        C=2.0\n    )\n    clf.fit(X[tr], y[tr])\n    p = clf.predict(X[va])\n    acc = accuracy_score(y[va], p)\n    f1 = f1_score(y[va], p)\n    fold_metrics.append((acc, f1))\n    oof_pred[va] = p\n    print(f\"[Fold {fold}] ACC={acc:.4f}  F1={f1:.4f}\")\n\nacc_mean = np.mean([m[0] for m in fold_metrics])\nf1_mean  = np.mean([m[1] for m in fold_metrics])\nprint(f\"\\nTF-IDF + LR 5-fold → ACC={acc_mean:.4f}  F1={f1_mean:.4f}\")\n\nprint(\"\\nClassification Report (OOF):\")\nprint(classification_report(y, oof_pred))\n\ncm = confusion_matrix(y, oof_pred)\ncm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:09:37.385001Z","iopub.execute_input":"2025-10-17T17:09:37.385639Z","iopub.status.idle":"2025-10-17T17:13:27.260465Z","shell.execute_reply.started":"2025-10-17T17:09:37.385615Z","shell.execute_reply":"2025-10-17T17:13:27.259603Z"}},"outputs":[{"name":"stdout","text":"[Fold 1] ACC=0.9031  F1=0.9038\n[Fold 2] ACC=0.9038  F1=0.9044\n[Fold 3] ACC=0.9023  F1=0.9031\n[Fold 4] ACC=0.9005  F1=0.9017\n[Fold 5] ACC=0.8999  F1=0.9008\n\nTF-IDF + LR 5-fold → ACC=0.9019  F1=0.9028\n\nClassification Report (OOF):\n              precision    recall  f1-score   support\n\n           0       0.91      0.89      0.90     25000\n           1       0.90      0.91      0.90     25000\n\n    accuracy                           0.90     50000\n   macro avg       0.90      0.90      0.90     50000\nweighted avg       0.90      0.90      0.90     50000\n\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([[22335,  2665],\n       [ 2239, 22761]])"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"\n## Model B — BiLSTM (5‑fold CV with EarlyStopping)\n\nWe tokenize **cleaned text**, pad sequences, and train a small **BiLSTM** per fold.\nTo keep runtime comfortable, we use few epochs (tune as you like).\n","metadata":{}},{"cell_type":"code","source":"\n# Tokenizer\nVOCAB_SIZE = 30000\nMAX_LEN    = 200\n\ntokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(df[\"text\"].tolist())\n\ndef build_bilstm():\n    model = Sequential([\n        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),\n        Bidirectional(LSTM(64, return_sequences=True)),\n        GlobalMaxPool1D(),\n        Dropout(0.2),\n        Dense(64, activation=\"relu\"),\n        Dropout(0.2),\n        Dense(1, activation=\"sigmoid\"),\n    ])\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model\n\ndef seqs_from_text(texts):\n    seqs = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n\nX_seq = seqs_from_text(df[\"text\"])\ny = df[\"label\"].values\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\noof_prob_lstm = np.zeros(len(df))\nfold_metrics_lstm = []\n\nEPOCHS = 3\nBATCH  = 1024\n\nfor fold, (tr, va) in enumerate(skf.split(X_seq, y), 1):\n    model = build_bilstm()\n    es = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True, verbose=1)\n    hist = model.fit(\n        X_seq[tr], y[tr],\n        validation_data=(X_seq[va], y[va]),\n        epochs=EPOCHS,\n        batch_size=BATCH,\n        verbose=1,\n        callbacks=[es]\n    )\n    prob = model.predict(X_seq[va], batch_size=2048).ravel()\n    oof_prob_lstm[va] = prob\n    pred = (prob >= 0.5).astype(int)\n    acc = accuracy_score(y[va], pred)\n    f1 = f1_score(y[va], pred)\n    fold_metrics_lstm.append((acc, f1))\n    print(f\"[BiLSTM Fold {fold}] ACC={acc:.4f}  F1={f1:.4f}\")\n\nacc_mean = np.mean([m[0] for m in fold_metrics_lstm])\nf1_mean  = np.mean([m[1] for m in fold_metrics_lstm])\nprint(f\"\\nBiLSTM 5-fold → ACC={acc_mean:.4f}  F1={f1_mean:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:25:44.851072Z","iopub.execute_input":"2025-10-17T17:25:44.851673Z","iopub.status.idle":"2025-10-17T17:27:22.925449Z","shell.execute_reply.started":"2025-10-17T17:25:44.851647Z","shell.execute_reply":"2025-10-17T17:27:22.924878Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\nI0000 00:00:1760721950.972838      37 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1760721957.094879     134 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 113ms/step - accuracy: 0.6470 - loss: 0.6500 - val_accuracy: 0.8432 - val_loss: 0.3769\nEpoch 2/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8607 - loss: 0.3360 - val_accuracy: 0.8829 - val_loss: 0.2828\nEpoch 3/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.9207 - loss: 0.2123 - val_accuracy: 0.8847 - val_loss: 0.2831\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n[BiLSTM Fold 1] ACC=0.8829  F1=0.8808\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 129ms/step - accuracy: 0.6068 - loss: 0.6659 - val_accuracy: 0.8184 - val_loss: 0.4448\nEpoch 2/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8378 - loss: 0.3909 - val_accuracy: 0.8858 - val_loss: 0.2830\nEpoch 3/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.9073 - loss: 0.2443 - val_accuracy: 0.8942 - val_loss: 0.2797\nRestoring model weights from the end of the best epoch: 3.\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step\n[BiLSTM Fold 2] ACC=0.8942  F1=0.8950\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 111ms/step - accuracy: 0.6277 - loss: 0.6617 - val_accuracy: 0.8208 - val_loss: 0.4267\nEpoch 2/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8532 - loss: 0.3627 - val_accuracy: 0.8872 - val_loss: 0.2843\nEpoch 3/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.9139 - loss: 0.2270 - val_accuracy: 0.8907 - val_loss: 0.2870\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n[BiLSTM Fold 3] ACC=0.8872  F1=0.8885\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6308 - loss: 0.6440 - val_accuracy: 0.8576 - val_loss: 0.3369\nEpoch 2/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8819 - loss: 0.2908 - val_accuracy: 0.8804 - val_loss: 0.3065\nEpoch 3/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - accuracy: 0.9283 - loss: 0.1941 - val_accuracy: 0.8838 - val_loss: 0.3195\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step\n[BiLSTM Fold 4] ACC=0.8804  F1=0.8808\nEpoch 1/3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 112ms/step - accuracy: 0.6077 - loss: 0.6616 - val_accuracy: 0.8215 - val_loss: 0.4259\nEpoch 2/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.8501 - loss: 0.3657 - val_accuracy: 0.8836 - val_loss: 0.2834\nEpoch 3/3\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - accuracy: 0.9170 - loss: 0.2220 - val_accuracy: 0.8908 - val_loss: 0.2852\nRestoring model weights from the end of the best epoch: 2.\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step\n[BiLSTM Fold 5] ACC=0.8836  F1=0.8832\n\nBiLSTM 5-fold → ACC=0.8857  F1=0.8856\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"\n## Simple Ensemble (Optional)\n\nWe can ensemble the two models by averaging probabilities (LR OOF is hard predictions; for a fair demo we\nkeep separate, but you can retrain LR to keep `predict_proba` per-fold and blend).\n","metadata":{}},{"cell_type":"code","source":"\n# For a thorough ensemble, retrain TF-IDF + LR to store predict_proba per fold.\n# Here, as a quick reference, we just show BiLSTM OOF metrics computed above.\npass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:27:35.710971Z","iopub.execute_input":"2025-10-17T17:27:35.711261Z","iopub.status.idle":"2025-10-17T17:27:35.715281Z","shell.execute_reply.started":"2025-10-17T17:27:35.711238Z","shell.execute_reply":"2025-10-17T17:27:35.714526Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"\n## Brief Error Analysis — Negation & Sarcasm\n\nWe’ll examine misclassifications (from TF‑IDF LR OOF vs. ground truth) and highlight **negations** and a few\nsimple **sarcasm cues** (quotes, “yeah right”, etc.).\n","metadata":{}},{"cell_type":"code","source":"\n# Collect a sample of misclassifications from TF-IDF model\nmis_idx = np.where(oof_pred != y)[0]\nsample_idx = np.random.RandomState(SEED).choice(mis_idx, size=min(12, len(mis_idx)), replace=False)\n\ndef has_negation(text):\n    return any(tok in NEGATION_TOKENS or \"n't\" in tok for tok in text.split())\n\nSARC_PATTERNS = [r'\\\".*\\\"', r\"yeah right\", r\"sure,\", r\"as if\", r\"/s\"]\n\ndef has_sarcasm(text):\n    tl = text.lower()\n    return any(re.search(pat, tl) for pat in SARC_PATTERNS)\n\nerr_rows = []\nfor i in sample_idx:\n    raw = df.loc[i, \"text\"]\n    true = int(y[i])\n    pred = int(oof_pred[i])\n    err_rows.append({\n        \"i\": int(i),\n        \"true\": true,\n        \"pred\": pred,\n        \"negation\": bool(has_negation(raw)),\n        \"sarcasm_hint\": bool(has_sarcasm(raw)),\n        \"snippet\": raw[:240] + (\"...\" if len(raw) > 240 else \"\"),\n    })\n\npd.DataFrame(err_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:27:39.735570Z","iopub.execute_input":"2025-10-17T17:27:39.735850Z","iopub.status.idle":"2025-10-17T17:27:39.753183Z","shell.execute_reply.started":"2025-10-17T17:27:39.735828Z","shell.execute_reply":"2025-10-17T17:27:39.752424Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"        i  true  pred  negation  sarcasm_hint  \\\n0   47794     1     0     False         False   \n1   36289     0     1     False         False   \n2    9540     0     1     False         False   \n3   29122     0     1     False         False   \n4   31901     1     0     False         False   \n5   49013     0     1     False         False   \n6   23169     0     1      True         False   \n7   20045     0     1     False         False   \n8   38887     1     0      True         False   \n9   27148     1     0     False         False   \n10  29078     0     1      True         False   \n11    870     0     1     False         False   \n\n                                              snippet  \n0   main criticism film namely macy suddenly look ...  \n1   voor een verloren soldaat lost soldier sad exa...  \n2   film capture short moment mother son rural rus...  \n3   acclaim director mervyn leroy put drama film c...  \n4   talk blast opening trampa infernal cool openin...  \n5   jason priestly star breakfast psychotic jewelr...  \n6   somerset maugham write novel coal miner decide...  \n7   lot matter helen none good shelley winter debb...  \n8   problem version movie simple indiana jones clo...  \n9   think crewe evil part well win award anything ...  \n10  fight scene great love old new cylon paint one...  \n11  person think entire forensic scene crime commu...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>i</th>\n      <th>true</th>\n      <th>pred</th>\n      <th>negation</th>\n      <th>sarcasm_hint</th>\n      <th>snippet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>47794</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>main criticism film namely macy suddenly look ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>36289</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>voor een verloren soldaat lost soldier sad exa...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9540</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>film capture short moment mother son rural rus...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>29122</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>acclaim director mervyn leroy put drama film c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>31901</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>talk blast opening trampa infernal cool openin...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>49013</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>jason priestly star breakfast psychotic jewelr...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>23169</td>\n      <td>0</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>somerset maugham write novel coal miner decide...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>20045</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>lot matter helen none good shelley winter debb...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>38887</td>\n      <td>1</td>\n      <td>0</td>\n      <td>True</td>\n      <td>False</td>\n      <td>problem version movie simple indiana jones clo...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>27148</td>\n      <td>1</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>think crewe evil part well win award anything ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>29078</td>\n      <td>0</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>fight scene great love old new cylon paint one...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>870</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>False</td>\n      <td>person think entire forensic scene crime commu...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\n\n# Fit on FULL cleaned text for demo inference\ntfidf_word_demo = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=200_000, sublinear_tf=True)\ntfidf_char_demo = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=2, max_features=100_000, sublinear_tf=True)\n\nXw_demo = tfidf_word_demo.fit_transform(df[\"text\"])\nXc_demo = tfidf_char_demo.fit_transform(df[\"text\"])\nfrom scipy.sparse import hstack\nX_demo = hstack([Xw_demo, Xc_demo]).tocsr()\n\nclf_demo = LogisticRegression(max_iter=1500, n_jobs=-1, solver=\"saga\", C=2.0)\nclf_demo.fit(X_demo, df[\"label\"].values)\n\nprint(\"Demo TF-IDF + LR are fitted (ready for .transform()).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:35:00.476058Z","iopub.execute_input":"2025-10-17T17:35:00.476363Z","iopub.status.idle":"2025-10-17T17:37:03.173947Z","shell.execute_reply.started":"2025-10-17T17:35:00.476342Z","shell.execute_reply":"2025-10-17T17:37:03.173237Z"}},"outputs":[{"name":"stdout","text":"Demo TF-IDF + LR are fitted (ready for .transform()).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"\n## Inference Helper\n\n`predict_texts(texts)` returns predictions for both pipelines (after fitting). For the BiLSTM, we reuse the\ntokenizer and the last trained fold’s weights for a quick demo.\n","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import hstack\nimport pandas as pd\n\ndef _transform_features_demo(cleaned_texts):\n    Xw = tfidf_word_demo.transform(cleaned_texts)\n    Xc = tfidf_char_demo.transform(cleaned_texts)\n    return hstack([Xw, Xc]).tocsr()\n\ndef predict_texts(texts):\n    cleaned = [preprocess_text(t) for t in texts]  # ← corrected bracket\n    Xnew = _transform_features_demo(cleaned)\n    y_tfidf = clf_demo.predict(Xnew)\n\n    # LSTM quick path (skip gracefully if model not available)\n    try:\n        seq = seqs_from_text(cleaned) if \"seqs_from_text\" in globals() else seqs(cleaned)\n        y_lstm = (lstm_demo.predict(seq) >= 0.5).astype(int).ravel()\n    except Exception:\n        y_lstm = None\n\n    return cleaned, y_tfidf, y_lstm\n\n# Quick test\nsamples = [\n    \"I absolutely loved this movie, the performances were brilliant.\",\n    \"Not good. I didn't enjoy it at all — boring and predictable.\"\n]\ncleaned, y_tf, y_lstm = predict_texts(samples)\npd.DataFrame({\"text\": samples, \"cleaned\": cleaned, \"tfidf_label\": y_tf, \"lstm_label\": y_lstm})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:37:33.966032Z","iopub.execute_input":"2025-10-17T17:37:33.966541Z","iopub.status.idle":"2025-10-17T17:37:34.078468Z","shell.execute_reply.started":"2025-10-17T17:37:33.966517Z","shell.execute_reply":"2025-10-17T17:37:34.077912Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  I absolutely loved this movie, the performance...   \n1  Not good. I didn't enjoy it at all — boring an...   \n\n                                       cleaned  tfidf_label  lstm_label  \n0  absolutely love movie performance brilliant            1           1  \n1                good enjoy boring predictable            0           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>cleaned</th>\n      <th>tfidf_label</th>\n      <th>lstm_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I absolutely loved this movie, the performance...</td>\n      <td>absolutely love movie performance brilliant</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Not good. I didn't enjoy it at all — boring an...</td>\n      <td>good enjoy boring predictable</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"\n## (Optional) Save/Load Artifacts\n\nBelow shows how to persist the TF‑IDF vectorizers & LR model, plus the Keras Tokenizer and BiLSTM weights.\n","metadata":{}},{"cell_type":"code","source":"\nimport joblib\n\nos.makedirs(\"artifacts\", exist_ok=True)\n\n# Save TF-IDF + LR\njoblib.dump(tfidf_word_demo, \"artifacts/tfidf_word.joblib\")\njoblib.dump(tfidf_char_demo, \"artifacts/tfidf_char.joblib\")\njoblib.dump(clf_demo, \"artifacts/logreg.joblib\")\n\n# Save Keras tokenizer & weights\nimport pickle\nwith open(\"artifacts/tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\nlstm_demo.save(\"artifacts/bilstm.h5\")\n\n!ls -lh artifacts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T17:37:42.105116Z","iopub.execute_input":"2025-10-17T17:37:42.105372Z","iopub.status.idle":"2025-10-17T17:37:53.848206Z","shell.execute_reply.started":"2025-10-17T17:37:42.105355Z","shell.execute_reply":"2025-10-17T17:37:53.847376Z"}},"outputs":[{"name":"stdout","text":"total 108M\n-rw-r--r-- 1 root root  46M Oct 17 17:37 bilstm.h5\n-rw-r--r-- 1 root root 2.3M Oct 17 17:37 logreg.joblib\n-rw-r--r-- 1 root root 7.3M Oct 17 17:37 tfidf_char.joblib\n-rw-r--r-- 1 root root  50M Oct 17 17:37 tfidf_word.joblib\n-rw-r--r-- 1 root root 3.4M Oct 17 17:37 tokenizer.pkl\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"\n### Notes & Tips\n\n- Tune `max_features`, `C`, `ngram_range` for TF‑IDF/LogReg, and `MAX_LEN`, `EMBEDDING_SIZE`, `LSTM_UNITS`, `EPOCHS`\n  for BiLSTM to trade off speed vs. quality.\n- Keep **negation tokens** in preprocessing; they matter a lot in sentiment!\n- For better deep models, consider pretrained embeddings (GloVe) or modern transformer baselines (e.g., DistilBERT).\n- Ensure reproducibility with seeds; results vary slightly by runtime/library versions.\n","metadata":{}}]}